{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: PQk-means\n",
    "This chapter contains the followings:\n",
    "\n",
    "1. Vector compression by Product Quantization\n",
    "1. Clustering by PQk-means\n",
    "1. Comparison to other clustering methods\n",
    "\n",
    "Requisites:\n",
    "- numpy\n",
    "- sklearn\n",
    "- pqkmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vector compression by Product Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pqkmeans\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First , we introduce vector compression by Product Quantization (PQ) [Jegou, TPAMI 11]. The first task is to train an encoder. Let us assume that there are 1000 six-dimensional vectors for training; $X_1 \\in \\mathbb{R}^{1000\\times6}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape:\n",
      "(1000, 6)\n",
      "\n",
      "X1:\n",
      "[[ 0.45915011  0.81146178  0.00506377  0.50923565  0.03212935  0.0972893 ]\n",
      " [ 0.18389336  0.85654138  0.28500285  0.50927319  0.22965493  0.24099061]\n",
      " [ 0.98793263  0.51580904  0.21431657  0.0530903   0.85640898  0.58706595]\n",
      " ..., \n",
      " [ 0.51872839  0.55220418  0.21158864  0.60338268  0.93821744  0.80093949]\n",
      " [ 0.40972777  0.81493828  0.12831053  0.07235744  0.89317441  0.99813665]\n",
      " [ 0.35910136  0.83867699  0.47266625  0.89194177  0.15437061  0.19536952]]\n"
     ]
    }
   ],
   "source": [
    "X1 = numpy.random.random((1000, 6))\n",
    "print(\"X1.shape:\\n{}\\n\".format(X1.shape))\n",
    "print(\"X1:\\n{}\".format(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train a PQEncoder using $X_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = pqkmeans.encoder.PQEncoder(num_subdim=2, Ks=256)\n",
    "encoder.fit(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder takes two parameters: $num\\_subdim$ and $Ks$. In the training step, each vector is splitted into $num\\_subdim$ sub-vectors, and quantized with $Ks$ codewords. The $num\\_subdim$ decides the bit length of PQ-code, and typically set as 4, 8, etc. The $Ks$ is usually set as 256 so as to represent each sub-code by $\\log_2 256=8$ bit.\n",
    "\n",
    "In this example, each 6D training vector is splitted into $num\\_subdim(=2)$ sub-vectors (two 3D vectors). Consequently, the 1000 6D training vectors are splitted into the two set of 1000 3D vectors. The k-means clustering is applied for each set of subvectors with $Ks=256$.\n",
    "\n",
    "Note that, alternatively, you can use `fit_generator` for a large dataset. This will be covered in the tutorial3.\n",
    "\n",
    "\n",
    "After the training step, the encoder stores the resulting codewords (2 subpspaces $*$ 256 codewords $*$ 3 dimensions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codewords.shape:\n",
      "(2, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"codewords.shape:\\n{}\".format(encoder.codewords.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can train the encoder preliminary using training data, and write/read the encoder via pickle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(encoder, open('encoder.pkl', 'wb'))  # Write\n",
    "# encoder = pickle.load(open('encoder.pkl', 'rb'))  # Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us consider database vectors (2000 six-dimensional vectors, $X_2$) that we'd like to compress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2.shape:\n",
      "(2000, 6)\n",
      "\n",
      "X2:\n",
      "[[ 0.15180971  0.80327565  0.59099579  0.00774046  0.16339874  0.3198269 ]\n",
      " [ 0.16263575  0.59685954  0.70140796  0.53771596  0.355683    0.38158597]\n",
      " [ 0.72568346  0.90049619  0.15491865  0.8290363   0.88244248  0.47203929]\n",
      " ..., \n",
      " [ 0.70051838  0.47091249  0.19485669  0.21541201  0.61178776  0.1875382 ]\n",
      " [ 0.02273475  0.20743724  0.79219709  0.30482205  0.01841295  0.36284165]\n",
      " [ 0.07808816  0.13373749  0.9020573   0.24226782  0.10198825  0.31281366]]\n",
      "\n",
      "Data type of each element:\n",
      "<class 'numpy.float64'>\n",
      "\n",
      "Memory usage:\n",
      "96000 byte\n"
     ]
    }
   ],
   "source": [
    "X2 = numpy.random.random((2000, 6))\n",
    "print(\"X2.shape:\\n{}\\n\".format(X2.shape))\n",
    "print(\"X2:\\n{}\\n\".format(X2))\n",
    "print(\"Data type of each element:\\n{}\\n\".format(type(X2[0][0])))\n",
    "print(\"Memory usage:\\n{} byte\".format(X2.nbytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compress these vectors by the trained PQ-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2_pqcode.shape:\n",
      "(2000, 2)\n",
      "\n",
      "X2_pqcode:\n",
      "[[ 13   8]\n",
      " [204 100]\n",
      " [168 125]\n",
      " ..., \n",
      " [  5 236]\n",
      " [ 92 175]\n",
      " [ 29 175]]\n",
      "\n",
      "Data type of each element:\n",
      "<class 'numpy.uint8'>\n",
      "\n",
      "Memory usage:\n",
      "4000 byte\n"
     ]
    }
   ],
   "source": [
    "X2_pqcode = encoder.transform(X2)\n",
    "print(\"X2_pqcode.shape:\\n{}\\n\".format(X2_pqcode.shape))\n",
    "print(\"X2_pqcode:\\n{}\\n\".format(X2_pqcode))\n",
    "print(\"Data type of each element:\\n{}\\n\".format(type(X2_pqcode[0][0])))\n",
    "print(\"Memory usage:\\n{} byte\".format(X2_pqcode.nbytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector is splitted into $num\\_subdim(=2)$ sub-vectors, and the nearest codeword is searched for each sub-vector. The id of the nearest codeword is recorded, i.e., two integers in this case. This representation is called PQ-code.\n",
    " \n",
    "PQ-code is a memory efficient data representation. The original 6D vector requies $6 * 64 = 384$ bit if 64 bit float is used for each element. On the other, a PQ-code requires only $2 * \\log_2 256 = 16$ bit. \n",
    "\n",
    "Note that we can approximately recunstruct the original vector from a PQ-code, by fetching the codewords using the PQ-code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original X2:\n",
      "[[ 0.15180971  0.80327565  0.59099579  0.00774046  0.16339874  0.3198269 ]\n",
      " [ 0.16263575  0.59685954  0.70140796  0.53771596  0.355683    0.38158597]\n",
      " [ 0.72568346  0.90049619  0.15491865  0.8290363   0.88244248  0.47203929]\n",
      " ..., \n",
      " [ 0.70051838  0.47091249  0.19485669  0.21541201  0.61178776  0.1875382 ]\n",
      " [ 0.02273475  0.20743724  0.79219709  0.30482205  0.01841295  0.36284165]\n",
      " [ 0.07808816  0.13373749  0.9020573   0.24226782  0.10198825  0.31281366]]\n",
      "\n",
      "reconstructed X2:\n",
      "[[ 0.08264026  0.7781403   0.71078617  0.06181019  0.11324808  0.29554268]\n",
      " [ 0.11838154  0.54206636  0.6747955   0.56273941  0.41778562  0.38644548]\n",
      " [ 0.70046182  0.94012103  0.15829005  0.72679207  0.84797079  0.536334  ]\n",
      " ..., \n",
      " [ 0.76580784  0.45700225  0.19958891  0.15874598  0.53537705  0.18357633]\n",
      " [ 0.06143844  0.08844606  0.75682856  0.25799713  0.11335996  0.33009549]\n",
      " [ 0.02050005  0.0332833   0.93969899  0.25799713  0.11335996  0.33009549]]\n"
     ]
    }
   ],
   "source": [
    "X2_reconstructed = encoder.inverse_transform(X2_pqcode)\n",
    "print(\"original X2:\\n{}\\n\".format(X2))\n",
    "print(\"reconstructed X2:\\n{}\".format(X2_reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the reconstructed vectors are similar to the original one.\n",
    "\n",
    "In a large-scale data processing scenario where all data cannot be stored on memory, you can compress input vectors to PQ-codes, and store the PQ-codes only (X2_pqcode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.save('pqcode.npy', X2_pqcode) # You can store the PQ-codes only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering by PQk-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the clustering over the PQ-codes. The clustering object is instanciated with the trained encoder. Here, we set the number of cluster as $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = pqkmeans.clustering.PQKMeans(encoder=encoder, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the PQk-means over X2_pqcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 1, 6, 5, 6, 7, 2, 6, 6, 2, 7, 2, 3, 4, 0, 6, 5, 7, 1, 0, 3, 7, 0, 6, 3, 8, 7, 9, 1, 7, 0, 7, 8, 1, 4, 4, 0, 7, 4, 1, 2, 7, 9, 6, 4, 0, 8, 4, 5, 5, 0, 5, 4, 2, 9, 1, 9, 9, 8, 1, 4, 0, 6, 2, 2, 4, 5, 1, 2, 5, 2, 9, 9, 1, 4, 1, 1, 9, 2, 1, 7, 9, 9, 1, 2, 8, 0, 8, 3, 6, 9, 7, 2, 7, 0, 7, 0, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "clustered = kmeans.fit_predict(X2_pqcode)\n",
    "print(clustered[:100]) # Just show the 100 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector (clustered) contains the id of assigned codeword for each input PQ-code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The id of assigned codeword for the 1st PQ-code is 3\n",
      "The id of assigned codeword for the 2nd PQ-code is 5\n",
      "The id of assigned codeword for the 3rd PQ-code is 1\n"
     ]
    }
   ],
   "source": [
    "print(\"The id of assigned codeword for the 1st PQ-code is {}\".format(clustered[0]))\n",
    "print(\"The id of assigned codeword for the 2nd PQ-code is {}\".format(clustered[1]))\n",
    "print(\"The id of assigned codeword for the 3rd PQ-code is {}\".format(clustered[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can fetch the center of the clustering by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering centers:[[150, 87], [138, 199], [80, 146], [91, 110], [206, 241], [166, 162], [93, 195], [34, 221], [77, 195], [238, 197]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"clustering centers:{}\\n\".format(kmeans.cluster_centers_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centers are also PQ-codes. They can be reconstructed by the PQ-encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructed clustering centers:\n",
      "[[ 0.67236277  0.17107652  0.53619635  0.73929783  0.49237796  0.47861757]\n",
      " [ 0.8055603   0.66486587  0.4045706   0.53686282  0.70065459  0.49703257]\n",
      " [ 0.19485406  0.12841325  0.44179731  0.66638549  0.39990629  0.52126625]\n",
      " [ 0.41317585  0.53819928  0.6209654   0.27129303  0.25384901  0.44805067]\n",
      " [ 0.65261487  0.77149634  0.73270011  0.74112421  0.11458843  0.41761175]\n",
      " [ 0.24689513  0.56747225  0.75825082  0.62652277  0.71499949  0.30156924]\n",
      " [ 0.35128772  0.35390192  0.24642649  0.23423394  0.59162514  0.58040981]\n",
      " [ 0.23566112  0.81520581  0.21010959  0.56226355  0.54656536  0.49115437]\n",
      " [ 0.43404732  0.32583786  0.79189525  0.23423394  0.59162514  0.58040981]\n",
      " [ 0.66513587  0.57157523  0.37975279  0.58714897  0.20513124  0.69112299]]\n"
     ]
    }
   ],
   "source": [
    "clustering_centers_numpy = numpy.array(kmeans.cluster_centers_, dtype=encoder.code_dtype)  # Convert to np.array with the proper dtype\n",
    "clustering_centers_reconstructd = encoder.inverse_transform(clustering_centers_numpy) # From PQ-code to 6D vectors\n",
    "print(\"reconstructed clustering centers:\\n{}\".format(clustering_centers_reconstructd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summalize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13th input vector:\n",
      "[ 0.23370251  0.34135083  0.03342117  0.91975159  0.78653989  0.32892066]\n",
      "\n",
      "13th PQ code:\n",
      "[ 63 223]\n",
      "\n",
      "reconstructed 13th PQ code:\n",
      "[ 0.22967011  0.29144491  0.08561297  0.97148021  0.81842     0.30072818]\n",
      "\n",
      "ID of the assigned center:\n",
      "2\n",
      "\n",
      "Assigned center (PQ-code):\n",
      "[80, 146]\n",
      "\n",
      "Assigned center (reconstructed):\n",
      "[ 0.19485406  0.12841325  0.44179731  0.66638549  0.39990629  0.52126625]\n"
     ]
    }
   ],
   "source": [
    "print(\"13th input vector:\\n{}\\n\".format(X2[12]))\n",
    "print(\"13th PQ code:\\n{}\\n\".format(X2_pqcode[12]))\n",
    "print(\"reconstructed 13th PQ code:\\n{}\\n\".format(X2_reconstructed[12]))\n",
    "print(\"ID of the assigned center:\\n{}\\n\".format(clustered[12]))\n",
    "print(\"Assigned center (PQ-code):\\n{}\\n\".format(kmeans.cluster_centers_[clustered[12]]))\n",
    "print(\"Assigned center (reconstructed):\\n{}\".format(clustering_centers_reconstructd[clustered[12]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can pickle the kmeans instace. The instance can be reused later as a vector quantizer for new input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(kmeans, open('kmeans.pkl', 'wb'))  # Write\n",
    "# kmeans = pickle.load(open('kmeans.pkl', 'rb'))  # Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison to other clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare PQk-means and the traditional k-means using high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3 = numpy.random.random((1000, 1024))  # 1K 1024-dim vectors, for training \n",
    "X4 = numpy.random.random((10000, 1024)) # 10K 1024-dim vectors, for database\n",
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the encoder\n",
    "encoder_large = pqkmeans.encoder.PQEncoder(num_subdim=4, Ks=256)\n",
    "encoder_large.fit(X3)\n",
    "\n",
    "# Encode the vectors to PQ-code\n",
    "X4_pqcode = encoder_large.transform(X4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the PQ-kmeans, and see the computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 340 ms, sys: 0 ns, total: 340 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%time clustered_pqkmeans = pqkmeans.clustering.PQKMeans(encoder=encoder_large, k=K).fit_predict(X4_pqcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the traditional k-means clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 s, sys: 68 ms, total: 1.87 s\n",
      "Wall time: 51.3 s\n"
     ]
    }
   ],
   "source": [
    "%time clustered_kmeans = KMeans(n_clusters=K, n_jobs=-1).fit_predict(X4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PQk-means would be tens to hundreds of times faster than k-means depending on your machine. Then let's see the accuracy. Since the result of PQk-means is the approximation of that of k-means, k-means achieved the lower error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQk-means, micro avg error: 9.175470609104421\n",
      "k-means, micro avg error: 9.151204550465865\n"
     ]
    }
   ],
   "source": [
    "_, pqkmeans_micro_average_error, _ = pqkmeans.evaluation.calc_error(clustered_pqkmeans, X4, K)\n",
    "_, kmeans_micro_average_error, _ = pqkmeans.evaluation.calc_error(clustered_kmeans, X4, K)\n",
    "\n",
    "print(\"PQk-means, micro avg error: {}\".format(pqkmeans_micro_average_error))\n",
    "print(\"k-means, micro avg error: {}\".format(kmeans_micro_average_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
