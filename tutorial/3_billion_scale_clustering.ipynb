{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Billion-scale clustering\n",
    "This chapter contains the followings:\n",
    "\n",
    "1. Download the SIFT1B dataset\n",
    "1. Encode billion-scale data iteratively\n",
    "1. Run clustering \n",
    "\n",
    "Requisites:\n",
    "- numpy\n",
    "- pqkmeans\n",
    "- tqdm\n",
    "- six\n",
    "- os\n",
    "- gzip\n",
    "- texmex_python (automatically installed when you pip pqkmeans)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the SIFT1B dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pqkmeans\n",
    "import tqdm\n",
    "import os\n",
    "import six\n",
    "import gzip\n",
    "import texmex_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we show an example of billion-scale clustering. Since input vectors are compressed by PQ, our PQk-means can handle a large amount of vectors even if they cannot be directly loaded on memory.\n",
    "\n",
    "In a programming perspective, our PQ-encoder has an iterative encoding function (`tranfsorm_generator`), by which we can handle large-scale data as if theyr are on memory.\n",
    "\n",
    "Let's start the tutorial by downloading the SIFT1B data. It consists of one billion 128-dimensional SIFT vectors, and requires 97.9 GB disk space. The download might take several hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_directory = \".\"  # Please set this according to your environment. 97.9 GB disk space is required.  \n",
    "filename = \"bigann_base.bvecs.gz\"\n",
    "url = \"ftp://ftp.irisa.fr/local/texmex/corpus/\" + filename\n",
    "path = os.path.join(cache_directory, filename)\n",
    "if not os.path.exists(path):\n",
    "    print(\"downloading {}\".format(url))\n",
    "    %time six.moves.urllib.request.urlretrieve(url, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's open the data and construct an iterator for it in a usual Python way. The `texmex_python` package contains an iterator-interface for `bvecs`-type data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = gzip.open(path, 'rb')\n",
    "vec_iterator = texmex_python.reader.read_bvec_iter(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can read each SIFT vector one by one using a usual for-loop access, e.g., \"`for v in vec_iterator: ...`\". Note that you do not need to read all data at once, which would require 97.9 GB of memory space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encode billion-scale data iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before encoding, let us construct a PQ-encoder using a small amount of training data. We use a traning data of SIFTSMALL dataset for the sake of simplicity (You should use training data of SIFT1B dataset for the evaluation, which takes 9.7 GB disk space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn_data, _ = pqkmeans.evaluation.get_siftsmall_dataset()\n",
    "M = 4\n",
    "encoder = pqkmeans.encoder.PQEncoder(num_subdim=M, Ks=256)\n",
    "encoder.fit(learn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll encode each SIFT vector to PQ-code iteratively. To do so, let us create a generator by calling `transform_generator` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pqcode_generator = encoder.transform_generator(vec_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `pqcode_generator` is a generator for PQ-code. We can encode each SIFT vector by, e.g., \"`for code in pqcode_generator: ...`\", without loading all data on memory at once.\n",
    "\n",
    "This design is not specific for SIFT1B data. Whenever you need to compress big data that cannot be loaded on memory at once, you can write an iterator for your data, and pass it to a PQ-encoder.\n",
    "\n",
    "So let's run encoding. To avoid consuming redundant memory space, we first allocate a big matrix as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pqcodes.shape:\n",
      "(1000000000, 4)\n",
      "pqcodes.nbytes:\n",
      "4000000000 bytes\n",
      "pqcodes.dtype:\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "N = 1000000000\n",
    "pqcodes = numpy.empty([N, M], dtype=encoder.code_dtype)\n",
    "print(\"pqcodes.shape:\\n{}\".format(pqcodes.shape))\n",
    "print(\"pqcodes.nbytes:\\n{} bytes\".format(pqcodes.nbytes))\n",
    "print(\"pqcodes.dtype:\\n{}\".format(pqcodes.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode vectors by simply running a usual for-loop statement. The encoding is automatically parallelized. You do not need to execute any additional steps. The encoding for the SIFT1B would take several hours depending on your computer. Please find that this does not consume any addirional memory cost at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000000/1000000000 [4:34:45<00:00, 60660.54it/s] \n"
     ]
    }
   ],
   "source": [
    "for n, code in enumerate(tqdm.tqdm(pqcode_generator, total=N)):\n",
    "    pqcodes[n, :] = code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it's also fine to use list comprehensions and numpy conversion such as \"`pqcodes=[code for code in pqcode_generator]\" and \"pqcodes=numpy.array(pqcodes)`\". But it would take memory overhead for temporal data storage.\n",
    "\n",
    "After encoding, you can save the pqcodes (and the PQ-encoder itself) if you want. Typically, the resulting PQ-codes do not take so much memory space (in this case, they take only 4 GB). So you can read/write the PQ-codes directly without any iterator/generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(encoder, open('encoder.pkl', 'wb'))\n",
    "# numpy.save('pqcode.npy', pqcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run clustering on one billion PQ-codes. The clustering for billion-scale data with `K=1000` is finished in several hours depending on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of clustering:\n",
      "CPU times: user 1d 1h 27min 35s, sys: 4min 16s, total: 1d 1h 31min 51s\n",
      "Wall time: 3h 41min 36s\n"
     ]
    }
   ],
   "source": [
    "K = 1000\n",
    "print(\"Runtime of clustering:\")\n",
    "%time clustered = pqkmeans.clustering.PQKMeans(encoder=encoder, k=K).fit_predict(pqcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The assigned label for the top 100 PQ-codes:\n",
      "[695, 981, 119, 124, 630, 471, 709, 287, 630, 240, 395, 721, 214, 769, 993, 742, 812, 742, 205, 812, 467, 26, 535, 709, 890, 699, 984, 446, 331, 984, 299, 947, 905, 874, 984, 962, 326, 366, 872, 26, 113, 1, 113, 321, 983, 295, 766, 4, 946, 399, 641, 164, 848, 71, 169, 415, 75, 205, 96, 144, 198, 671, 131, 4, 706, 859, 152, 459, 623, 361, 991, 38, 723, 400, 941, 15, 190, 840, 638, 371, 140, 805, 56, 345, 578, 442, 250, 779, 333, 111, 585, 974, 52, 52, 991, 12, 415, 408, 819, 54]\n"
     ]
    }
   ],
   "source": [
    "print(\"The assigned label for the top 100 PQ-codes:\\n{}\".format(clustered[:100]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
